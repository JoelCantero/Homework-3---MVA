---
title: "Practice Clustering and Profiling"
author: "Marc Mendez & Joel Cantero"
date: "7 de abril de 2019"
output: html_document
---

```{r setup, include=FALSE}
library(FactoMineR)
library(mice)
library(ggplot2)
library(dendextend)
library(calibrate)
library(factoextra)
library(fpc)
```

## 1. Read the pre-processed data "Russet". Perform the Principal Components Analysis. Take the democracy index as supplementary variable, whereas the remaining ones are active and CUBA as supplementary individual (you can synthesize results of previous homework).

First of all, we are going to read the data and pre-process it correctly. We need to set demo as a factor and can input the levels but this won't have any impact. This steps are the same we made on last homework.

The once data is properly set, we need to input the missing values. For doing that we will use mice function.

```{r read-table}
X <- read.table('Russet_ineqdata.txt', header=T, sep='\t', row.names=1)
X$demo <- as.factor(X$demo)
levels(X$demo) = c("Stable", "Instable", "Dictatorship")
imputedX <- complete(mice(X))
row.names(imputedX) <- row.names(X)
```

As known from the other practices, CUBA is an outlier so we will consider this when doing the PCA.
Now, we are going to call PCA FactoMineR function and we are going to indicate Cuba as a supplementary individual (ind.sup = 11 (Cuba)), democracy index as a supplemtary variable (quali.sup=9 (demo))

```{r pca}
pca <- FactoMineR::PCA(graph=T, imputedX, ncp=8, quali.sup=9, ind.sup = 11)
```

## 2. Interpret the first two obtained factors.


```{r interpret two-obtained-factors}
eig = as.data.frame(pca$eig)
eigenv = eig$eigenvalue
eigenv
eig
```

The first dimension tries to define the variance of those countries that are farmers FARM with a negative GNPR per capita, like Colombia.

The second one takes GINI positively and LABO negatively. That mean that are countries with less people than average working and that the inequalities of capital in the population are high.

## 3. Decide the number of significant dimensions that you retain (by subtracting the average eigenvalue and represent the new obtained eigenvalues in a new screeplot).

```{r interpret two-obtained-factors2}
screen_frame <- data.frame(Number = seq(1, length(eigenv)), Value = eigenv)
plot(screen_frame, main="Eigen values screeplot",
     xlab="Dimension", ylab="Info. retained" , ylim=c(0, max(eigenv) + 0.5), xlim=c(1, length(eigenv) + 0.5))  +
     abline(h = 1, col = 'red')
lines(screen_frame$Value, col="blue")
textxy(screen_frame$Number, screen_frame$Value, round(screen_frame$Value, 2), cex=1.1)
```

With the obtained screeplot and applying the last ebow rule we can select the first three dimensions as significant ones. And if we check the eigen variable there is a column with the percentatges of each component. With this three variables, the percentatge of information that can be explained is 70.68817%.

## 4. Perform a hierarchical clustering with the significant factors, decide the number of final classes to obtain and perform a consolidation operation of the clustering.

As we mentioned before, we are going to select three significant factors. 

```{r hierarchical clustering}
nd = 3  # number of dimensions
Psi <- pca$ind$coord[, 1:nd]
dist_matrix = dist(Psi)

cluster <- hclust(dist_matrix, method='ward.D2')
plot(cluster)
barplot(cluster$height)
```

Seeing the dendogram and the barplot, we can check that there are 4 different heights in the plot. So we select the first 4 classes.

```{r clusters}
clusters = 4
c1 <- cutree(cluster, clusters)
plot(Psi,type="n",main="Clustering of countries in 4 classes")
text(Psi,col=c1,labels=rownames(Psi),cex = 0.6)
abline(h=0,v=0,col="gray")
legend("bottomright",c("Class 1","Class 2","Class 3","Class 4"),pch=20,col=c(1:4))

c2 <- cutree(cluster, clusters)
colors = hsv(c(0.3, 0.75, 0.95, 0.5), 1, 0.8)
dend <- as.dendrogram(cluster)
dend <- dend %>%
    color_branches(k = clusters, col=colors) %>%
    set("branches_lwd", c(2,1,2)) %>%
    set("branches_lty", c(1,2,1))

plot(dend); (cdg <- aggregate(Psi,list(c2),mean)[,2:(nd+1)])
k_def <- kmeans(Psi,centers=cdg)
```

## 5 Compute the Calinski-Harabassz index and compare before and after the consolidation step.

Before making the consolidation step we obtaing a Calinski-Harabassz index of 28.

```{r calinhara-Before}
calinhara(Psi,c2)
```
After making all the consolidation, we obtain almost 30 which makes sense because kmeans should make this index bigger.
```{r calinhara-After}
calinhara(Psi,k_def$cluster)
```

## 6. Using the function catdes interpret and name the obtained clusters and represent them in the first factorial display.
```{r catdes}
clusters_interpretation <- catdes(cbind(as.factor(k_def$cluster), imputedX[-11,]),1, 0.05)
plot(clusters_interpretation)
plot(pca$ind$coord, col=k_def$cluster)
```


We can intepret the outcomes all the more effectively by utilizing the plot of the catdes, which demonstrates which factors are under or over spoken to in our clusters, and to which deegre are they over or under represented.

It appears that the main cluster has a considerable amount of overrepresented information (appeared red), this is genuine regardless of what number of groups do we attempt, which appears to infer this is an issue which can not be fixed, still different groups appear to have a decent generally speaking portrayal of the factors. We can not look at the value of *p-value* due to all of them being lower than *0.05*, it is better if we look at *v.test*.

## 7. Which is the most plausible profile for CUBA? Why?
```{r cuba-profile}
distance <- apply(k_def$centers, 1, function(i) (sum((pca$ind.sup$coord[1:nd] - i)^2)))
max.col(-t(distance))
```

We have to calculate the equilidan distance between the centroids an the points. The cluster which is more close to is cluster number 3.
